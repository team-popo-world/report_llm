from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from langchain_qdrant import QdrantVectorStore
from langchain_community.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from utils.load_db import load_userId
from utils.invest_join import update_invest_data
from utils.quest_join import update_quest_data
from utils.shop_join import update_shop_data
from utils.find_my_child import find_child_info
from utils.cluster_count import transform_cluster_counts
from utils.json_to_natural_language import format_natural_language_summary
import requests
import pandas as pd
import os
from dotenv import load_dotenv
from pymongo import MongoClient
import json
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
url = os.getenv("QDRANT_URL")

client = QdrantClient(url=url, prefer_grpc=False)

# SQLì—ì„œ ì „ì²´ ë°ì´í„° ë°›ì•„ì˜¤ê¸°
#user_list = load_userId()

user_list = ["237aac1b-4d6f-4ca9-9e4f-30719ea5967d", "d4af0657-f9db-40ff-babd-68681db7ddeb", "3ed8a159-adb0-4380-a648-e015cb82690a", "22b19e79-5822-46f9-8e3f-7e55e751f9dc", "d0a188a3-e24e-4772-95f7-07e59ce8885e", "4671ffa6-c77e-45f5-af8e-0f1d80804d86", "8a2b94b0-8395-41ad-99b0-6e8c0ea7edaa"]

# user_list = ["237aac1b-4d6f-4ca9-9e4f-30719ea5967d", "956f51a8-d6a0-4a12-a22b-9da3cdffc879", "f0220d43-513a-4619-973d-4ed84a42bf6a", "d0a188a3-e24e-4772-95f7-07e59ce8885e"]
# userId = "237aac1b-4d6f-4ca9-9e4f-30719ea5967d"

# invest_merged_df = pd.DataFrame()  # ì´ˆê¸° ë³‘í•©ìš© ë°ì´í„°í”„ë ˆì„
# shop_merged_df = pd.DataFrame()

# for userId in user_list:
#    invest_merged_df = update_invest_data(userId, invest_merged_df)

# quest_merged_df = update_quest_data(user_list)
# shop_merged_df = update_shop_data(user_list)

# invest_merged_df.to_csv("data/invest_merged_df.csv", index=False)
# quest_merged_df.to_csv("data/quest_merged_df.csv", index=False)
#shop_merged_df.to_csv("data/shop_merged_df_test.csv", index=False)

invest_merged_df = pd.read_csv("data/invest_merged_df.csv")
quest_merged_df = pd.read_csv("data/quest_merged_df.csv")
shop_merged_df = pd.read_csv("data/shop_merged_df_test.csv")

cluster_df = transform_cluster_counts(invest_merged_df)
print(cluster_df.info())

# # ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ
#invest_merged_df.to_csv("data/invest_merged_df.csv", index=False)
#invest_merged_df = pd.read_csv("data/invest_merged_df.csv")
#quest_df = pd.read_csv("data/Final_Quest_DataFrame.csv")
#shop_df = pd.read_csv("data/shop_data_sample.csv")

# invest_merged_df = format_natural_language_summary(invest_merged_df)

# # ğŸ“ ì €ì¥
# formatted_df.to_csv("data/natural_format_data.csv", index=False)

# format_df = pd.read_csv("data/natural_format_data.csv")

# documents = [
#     Document(
#         page_content=row["formatText"],
#         metadata={"userId": row["userId"]}
#     )
#     for _, row in formatted_df.iterrows()
# ]

# Document(
#     page_content=f"""
#     ì‚¬ìš©ì ID: {row['userId']}
#     ê²Œì„ ì‹œì‘ ì‹œê°: {row['startedAt']}
#     ìš”ì•½ ì„¤ëª…: {row['formatText']}
#     """.strip(),
#     metadata={"userId": row["userId"]}
# )

# embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# # ë²¡í„° ì°¨ì› ì•Œì•„ë‚´ê¸°
# embedding_dim = len(embedding.embed_query("ì„ë² ë”© í…ŒìŠ¤íŠ¸"))

# client.delete_collection(collection_name="my_user_summaries")

# ì¡°ê±´ë¶€ ìƒì„± (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
# if not client.collection_exists("my_user_summaries"):
#     client.create_collection(
#         collection_name="my_user_summaries",
#         vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)
#     )

# ë¬¸ì„œ ìˆœíšŒí•˜ë©° ì—…ì„œíŠ¸
# for doc in documents:
#     user_id = doc.metadata["userId"]
#     vector = embedding.embed_query(doc.page_content)
    
#     # Qdrantì— ì—…ì„œíŠ¸
#     client.upsert(
#         collection_name="my_user_summaries",
#         points=[
#             PointStruct(
#                 id=user_id,  # ë¬¸ìì—´ UUIDë¥¼ ê·¸ëŒ€ë¡œ IDë¡œ ì‚¬ìš©
#                 vector=vector,
#                 payload={**doc.metadata, "text": doc.page_content}
#             )
#         ]
#     )

# Qdrant ë²¡í„° DB ìƒì„± ë° ì €ì¥
# qdrant = QdrantVectorStore(
#     client=client,
#     embedding=embedding,
#     collection_name="my_user_summaries"
# )

# qdrant.add_documents(documents)

# ë‚´ ì•„ì´ì˜ userId
# target_user_id = "237aac1b-4d6f-4ca9-9e4f-30719ea5967d"

# Qdrantì—ì„œ í•´ë‹¹ userIdë¡œ point ì¡°íšŒ
# user_data = find_child_info(target_user_id, client)
# response = client.retrieve(
#     collection_name="my_user_summaries",
#     ids=[target_user_id] # ID ê¸°ë°˜ ì¡°íšŒ
# )

template = """
ë„ˆëŠ” ì•„ì´ë“¤ì˜ ê²Œì„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ í–‰ë™ì„ íŒŒì•…í•˜ê³ , ê° ì˜ì—­ë³„ë¡œ **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë¹„êµë¥¼ í†µí•´ í”¼ë“œë°±ê³¼ ê°€ì´ë“œë¥¼ ì œì‹œí•˜ëŠ” AI í•™ìŠµ ë¶„ì„ê°€**ì•¼.

ë¶„ì„ ëŒ€ìƒì€ í•­ìƒ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ í™œë™ ì˜ì—­ì´ì•¼:
- íˆ¬ì(invest)
- ìƒì (shop)
- í€˜ìŠ¤íŠ¸(quest)

ê° ì˜ì—­ì—ëŠ” ìˆ«ì ê¸°ë°˜ ì§€í‘œë“¤ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ, **ìˆ˜ì¹˜ì— ê¸°ë°˜í•˜ì—¬ ê°ê´€ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…**í•´ì¤˜. ì˜ˆë¥¼ ë“¤ì–´ 'í™œë™ëŸ‰ì´ 2.0ì´ê³  ì „ì²´ í‰ê· ì€ 1.5'ë¼ë©´, 'ì „ì²´ë³´ë‹¤ ì•½ 33% ë†’ì€ í™œë™ëŸ‰'ì´ë¼ê³  ì„œìˆ í•˜ëŠ” ì‹ì´ì•¼.

ê° ì˜ì—­ë§ˆë‹¤ ì•„ë˜ í•­ëª©ì„ ëª¨ë‘ í¬í•¨í•œ **ìš”ì•½ í…ìŠ¤íŠ¸**ë¥¼ ìƒì„±í•´:
1. í™œë™ëŸ‰ ë˜ëŠ” ë¹ˆë„ ìˆ˜ì¹˜ì™€ ì˜ë¯¸
2. í–‰ë™ì˜ íŠ¹ì„± (ì˜ˆ: ê³ ìœ„í—˜ ì„ í˜¸, ì†Œë¹„ íŒ¨í„´, ì„±ê³µë¥  ë“±)
3. ì „ì²´ í‰ê· ê³¼ ë¹„êµí•œ ìˆ˜ì¹˜ ë° í•´ì„
4. ê°œì„ í•  ì  (ì™œ ê°œì„ ì´ í•„ìš”í•œì§€ë„ ê°„ë‹¨íˆ ì„¤ëª…)
5. ì¹­ì°¬í•  ì  (ìˆ˜ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì¹­ì°¬)

ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë“  ì˜ì—­ì„ ì¢…í•©í•´ì„œ:
- ì•„ì´ì˜ **ì „ë°˜ì ì¸ í–‰ë™ ê²½í–¥**ì„ ì •ë¦¬í•˜ê³ ,
- **í•™ìŠµ ë° ì¬ë¬´ ìŠµê´€ì— ëŒ€í•œ ì¢…í•© ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•´ì¤˜.

ğŸ”¹ ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒì˜ JSON êµ¬ì¡°ë¡œ í•´ì¤˜. ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´.
```json
{{
  "userId": "{user_id}",
  "invest": "íˆ¬ì ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "shop": "ìƒì  ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "quest": "í€˜ìŠ¤íŠ¸ ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "all": "ì „ì²´ í–‰ë™ ê²½í–¥ ë° í•™ìŠµ ê°€ì´ë“œ ìš”ì•½"
}}

[íˆ¬ì í™œë™ ë°ì´í„°]
{invest_data}

[ìƒì  í™œë™ ë°ì´í„°]
{shop_data}

[í€˜ìŠ¤íŠ¸ í™œë™ ë°ì´í„°]
{quest_data}
"""

# [ìƒì  í™œë™ ë°ì´í„°]
# {shop_data}

# [í€˜ìŠ¤íŠ¸ í™œë™ ë°ì´í„°]
# {quest_data}
"""
prompt = PromptTemplate.from_template(template)

openai_key = os.getenv("OPENAI_KEY")

# ëª¨ë¸ ì •ì˜
llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    streaming=True,
    temperature=0.8,
    openai_api_key=openai_key,
    callbacks=[StreamingStdOutCallbackHandler()]
)

chain = LLMChain(prompt=prompt, llm=llm)
"""



# response = chain.run({
#     "user_id": target_user_id,
#     "invest_data": invest_df,
#     "shop_data": shop_df,
#     "quest_data": quest_df
# })

# print("ğŸ§¾ ëª¨ë¸ ì‘ë‹µ ì›ë¬¸:", repr(response))

# # 1. Markdown í¬ë§· ì œê±° (ì˜ˆ: ```json\n ... \n```)
# cleaned = re.sub(r"^```(?:json)?\n|\n```$", "", response.strip())

# MongoDB ì—°ê²°
uri = os.getenv("MONGO_URI")
db_name = os.getenv("MONGO_DB_NAME")

client = MongoClient(uri)  # ë˜ëŠ” Atlas URI
db = client[db_name]
user_collection = db["user_analysis"]
graph_collection = db["user_graph"]


# try:
#     response_json = json.loads(cleaned)
# except json.JSONDecodeError as e:
#     print("â— JSON ë””ì½”ë”© ì‹¤íŒ¨:", e)
#     response_json = {}

# # userId ì¤‘ë³µ ë°©ì§€: ê¸°ì¡´ì— ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
# if response_json:
#     user_id = response_json.get("userId")
#     collection.update_one(
#         {"userId": user_id},
#         {"$set": response_json},
#         upsert=True
#     )
#     print(f"âœ… MongoDBì— ì €ì¥ ì™„ë£Œ (userId: {user_id})")

###########################################################################
# âœ… ì „ì²´ ì‚¬ìš©ìì— ëŒ€í•´ ë°˜ë³µ ì‹¤í–‰
"""
for user_id in user_list:
    try:
        print(f"ğŸš€ ë¶„ì„ ì¤‘: {user_id}")

        # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ë¨¼ì € í™•ì¸
        for df_name, df, col in [
            ("invest_merged_df", invest_merged_df, "userId"),
            ("cluster_df", cluster_df, "userId"),
            ("quest_merged_df", quest_merged_df, "child_id"),
            ("shop_merged_df", shop_merged_df, "userId")
        ]:
            if col not in df.columns:
                print(f"â— ì˜¤ë¥˜: {df_name}ì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (userId={user_id})")
                raise KeyError(f"{df_name} missing column {col}")

        invest_df = invest_merged_df[invest_merged_df["userId"] == user_id]
        cluster_user_df = cluster_df[cluster_df["userId"] == user_id]
        quest_df = quest_merged_df[quest_merged_df["child_id"] == user_id]
        shop_df = shop_merged_df[shop_merged_df["userId"] == user_id] 

        # ì²´ì¸ ì‹¤í–‰
        response = chain.run({
            "user_id": user_id,
            "invest_data": invest_df,
            "shop_data": shop_df,
            "quest_data": quest_df
        })

        # ì¶œë ¥ í´ë¦°ì—…
        cleaned = re.sub(r"^```(?:json)?\n|\n```$", "", response.strip())

        # JSON íŒŒì‹±
        response_user_json = json.loads(cleaned)

        cluster_df.drop(columns="userId", inplace=True)
        quest_df.drop(columns="child_id", inplace=True)
        # quest_df = quest_df[["userId", "", "", "", ""]]
        shop_df = shop_df[["weeklyTrend"]]

        graph_json = {
            "invest_graph": cluster_df.to_dict(orient="records"),
            "shop_graph": shop_df.to_dict(orient="records"),
            "quest_graph": quest_df.to_dict(orient="records"),
        }

        # MongoDBì— ì €ì¥
        user_collection.update_one(
            {"userId": user_id},
            {"$set": response_user_json},
            upsert=True
        )

        graph_collection.update_one(
            {"userId": user_id},
            {"$set": graph_json},
            upsert=True
        )

        print(f"âœ… ì €ì¥ ì™„ë£Œ: {user_id}")

    except Exception as e:
        print(f"â— ì˜¤ë¥˜ ë°œìƒ (userId={user_id}): {e}")

"""