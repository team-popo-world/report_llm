from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from langchain_qdrant import QdrantVectorStore
from utils.load_db import load_userId
from utils.json_to_natural_language import format_natural_language_summary
# from utils.json_to_documents import json_to_documents
import requests
import pandas as pd
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
host = os.getenv("QDRANT_HOST")
port = os.getenv("QDRANT_PORT")
url = os.getenv("QDRANT_URL")
# port = os.getenv("QDRANT_PORT")

# print(f"Qdrant ì—°ê²°: {host}: {port}")

# â¶ Qdrant ì„œë²„ ì—°ê²° (ì„œë²„ ì£¼ì†Œë¥¼ ë°”ê¾¸ì„¸ìš”)

client = QdrantClient(url=url, prefer_grpc=False)

# ì°¸ê³ ìë£Œ dagë¥¼ ì´ìš©í•´ì„œ ìš°ë¦¬ ì•„ì´ì˜ ê²°ê³¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë ˆí¬íŠ¸ ì‘ì„±í•˜ê¸°

# ì „ì²´ ìœ ì €ì— ëŒ€í•´ì„œ updateí•´ì•¼ í•˜ë¯€ë¡œ sqlì—ì„œ userIdë§Œ ë¶ˆëŸ¬ì™€ì„œ ì €ì¥
# userId(str)
# report_data(json)

# SQLì—ì„œ ì „ì²´ ë°ì´í„° ë°›ì•„ì˜¤ê¸°
user_list = load_userId()

print(len(user_list))

"""
ë°ì´í„° í”„ë ˆì„
# userId
# type(str, ì–´ë–¤ ë¶„ì•¼ì¸ì§€( ex)invest, quest, ...))
# graph_name(str; ì–´ë–¤ ê·¸ë˜í”„ì¸ì§€)
# graph_data(json; ê·¸ë˜í”„ ê·¸ë¦¬ëŠ”ë° ì‚¬ìš©í•œ jsoníŒŒì¼)
# graph_summary(str; ê·¸ë˜í”„ í¬ë§·íŒ…í•œ ê²°ê³¼)
"""

# apië¡œ ê·¸ë˜í”„ ë³„ë¡œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
graph_list = ["avg_stay_time", "buy_ratio", "sell_ratio", "buy_sell_ratio", "bet_ratio", "avg_cash_ratio"]

invest_merged_df = pd.DataFrame()  # ì´ˆê¸° ë³‘í•©ìš© ë°ì´í„°í”„ë ˆì„

##########################################í…ŒìŠ¤íŠ¸ìš©#############################################
# userId = "956f51a8-d6a0-4a12-a22b-9da3cdffc879"
# for graphName in graph_list:
#     invest_url = f"http://43.203.175.69:8002/api/invest/{graphName}/week?userId={userId}"
#     headers = {"Content-Type": "application/json"}

#     response = requests.get(invest_url, headers=headers)

#     if response.status_code == 200:
#         # print(response.json())  # ì‘ë‹µ ë°ì´í„°
#         data = response.json()  # JSON -> Python ê°ì²´ (list of dict)
#         df = pd.DataFrame(data)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ DataFrameìœ¼ë¡œ ë³€í™˜
#         print(df.head())  # í™•ì¸ìš©
#         if df.empty:
#             print(f"ğŸ“­ [EMPTY] userId: {userId}, graph: {graphName}")
#             continue  # ì•„ë¬´ê²ƒë„ ì•ˆí•˜ê³  ë‹¤ìŒìœ¼ë¡œ

#         print(f"âœ… [RECEIVED] userId: {userId}, graph: {graphName}, rows: {len(df)}")

#         # ë³‘í•© ì²˜ë¦¬
#         if invest_merged_df.empty:
#             invest_merged_df = df
#         else:
#             invest_merged_df = pd.merge(invest_merged_df, df, on=["userId", "startedAt"], how="inner")

#     else:
#         print("ì—ëŸ¬ ë°œìƒ:", response.status_code)
#         print("ì—ëŸ¬ ë‚´ìš©:", response.text)

# print(invest_merged_df.head())
# print("invest_merged_df columns:", invest_merged_df.columns)
# invest_merged_df.to_csv("data/íŠ¹ì •ì‚¬ëŒ_invest_api_ë¶ˆëŸ¬ì™€ì„œ_ë³‘í•©.csv", index=False, encoding="utf-8")
#####################################################################################################

# invest ìª½ api ë¶ˆëŸ¬ì˜¤ê¸°
for userId in user_list:
    for graphName in graph_list:
            invest_url = f"http://43.203.175.69:8002/api/invest/{graphName}/week?userId={userId}"
            headers = {"Content-Type": "application/json"}

            response = requests.get(invest_url, headers=headers)

            if response.status_code == 200:
                # print(response.json())  # ì‘ë‹µ ë°ì´í„°
                data = response.json()  # JSON -> Python ê°ì²´ (list of dict)
                df = pd.DataFrame(data)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ DataFrameìœ¼ë¡œ ë³€í™˜
                print(df.head())  # í™•ì¸ìš©
                if df.empty:
                    print(f"ğŸ“­ [EMPTY] userId: {userId}, graph: {graphName}")
                    continue  # ì•„ë¬´ê²ƒë„ ì•ˆí•˜ê³  ë‹¤ìŒìœ¼ë¡œ

                print(f"âœ… [RECEIVED] userId: {userId}, graph: {graphName}, rows: {len(df)}")

                # ë³‘í•© ì²˜ë¦¬
                if invest_merged_df.empty:
                    invest_merged_df = df
                else:
                    invest_merged_df = pd.merge(invest_merged_df, df, on=["userId", "startedAt"], how="inner")

            else:
                print("ì—ëŸ¬ ë°œìƒ:", response.status_code)
                print("ì—ëŸ¬ ë‚´ìš©:", response.text)

# quest_url = f"http://43.203.175.69:8000/graph/{}/{}?userId={userId}"
# shop_url = f"http://43.203.175.69:8001/api/{}/{}?userId={userId}"

# invest_merged_df = pd.read_csv("data/íŠ¹ì •ì‚¬ëŒ_invest_api_ë¶ˆëŸ¬ì™€ì„œ_ë³‘í•©.csv")

# # ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ
formatted_df = format_natural_language_summary(invest_merged_df)

# # ğŸ“ ì €ì¥
# formatted_df.to_csv("data/natural_format_data.csv", index=False)

# format_df = pd.read_csv("data/natural_format_data.csv")

documents = [
    Document(
        page_content=row["formatText"],
        metadata={"userId": row["userId"]}
    )
    for _, row in formatted_df.iterrows()
]

# Document(
#     page_content=f"""
#     ì‚¬ìš©ì ID: {row['userId']}
#     ê²Œì„ ì‹œì‘ ì‹œê°: {row['startedAt']}
#     ìš”ì•½ ì„¤ëª…: {row['formatText']}
#     """.strip(),
#     metadata={"userId": row["userId"]}
# )

embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# ë²¡í„° ì°¨ì› ì•Œì•„ë‚´ê¸°
embedding_dim = len(embedding.embed_query("ì„ë² ë”© í…ŒìŠ¤íŠ¸"))

# client.delete_collection(collection_name="my_user_summaries")

# ì¡°ê±´ë¶€ ìƒì„± (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
if not client.collection_exists("my_user_summaries"):
    client.create_collection(
        collection_name="my_user_summaries",
        vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)
    )

# ë¬¸ì„œ ìˆœíšŒí•˜ë©° ì—…ì„œíŠ¸
for doc in documents:
    user_id = doc.metadata["userId"]
    vector = embedding.embed_query(doc.page_content)
    
    # Qdrantì— ì—…ì„œíŠ¸
    client.upsert(
        collection_name="my_user_summaries",
        points=[
            PointStruct(
                id=user_id,  # ë¬¸ìì—´ UUIDë¥¼ ê·¸ëŒ€ë¡œ IDë¡œ ì‚¬ìš©
                vector=vector,
                payload={**doc.metadata, "text": doc.page_content}
            )
        ]
    )

# Qdrant ë²¡í„° DB ìƒì„± ë° ì €ì¥
qdrant = QdrantVectorStore(
    client=client,
    embedding=embedding,
    collection_name="my_user_summaries"
)

# qdrant.add_documents(documents)

query = "ê³ ìœ„í—˜ ìì‚° ì„ í˜¸í•˜ëŠ” ì•„ì´ë“¤"
results = qdrant.similarity_search(query, k=5)

for i, doc in enumerate(results, start=1):
    print(f"\nğŸ“Œ [TOP {i}]")
    print("ğŸ” userId:", doc.metadata.get("userId", "ì—†ìŒ"))
    print("ğŸ§¾ ë‚´ìš©:", doc.page_content.strip()[:300]) 

# json = load_mongo(collection_name="graph1_all_history")

# documents = json_to_documents(json)

# # ë²¡í„° DBì— ì €ì¥
# vector_db = Chroma.from_documents(
#   documents, 
#   embeddings=embeddings, 
#   persist_directory="./chroma_db"
# )

# # ì´í›„ RAGì— í™œìš©
# retriever = vector_db.as_retriever()
# retriever.get_relevant_documents("ìµœê·¼ íˆ¬ì íŠ¸ë Œë“œëŠ”?")