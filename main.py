from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from langchain_qdrant import QdrantVectorStore
from langchain_community.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from utils.load_db import load_userId
from utils.call_api import update_data
from utils.find_my_child import find_child_info
from utils.json_to_natural_language import format_natural_language_summary
import requests
import pandas as pd
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
url = os.getenv("QDRANT_URL")

client = QdrantClient(url=url, prefer_grpc=False)

# SQLì—ì„œ ì „ì²´ ë°ì´í„° ë°›ì•„ì˜¤ê¸°
# user_list = load_userId()

# print(len(user_list))

user_list = ["237aac1b-4d6f-4ca9-9e4f-30719ea5967d", "956f51a8-d6a0-4a12-a22b-9da3cdffc879", "f0220d43-513a-4619-973d-4ed84a42bf6a", "d0a188a3-e24e-4772-95f7-07e59ce8885e"]

# apië¡œ ê·¸ë˜í”„ ë³„ë¡œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
graph_list = ["avg_stay_time", "buy_ratio", "sell_ratio", "buy_sell_ratio", "bet_ratio", "avg_cash_ratio"]
# userId = "956f51a8-d6a0-4a12-a22b-9da3cdffc879"

invest_merged_df = pd.DataFrame()  # ì´ˆê¸° ë³‘í•©ìš© ë°ì´í„°í”„ë ˆì„

# for userId in user_list:
#     update_data(userId, invest_merged_df)

# invest ìª½ api ë¶ˆëŸ¬ì˜¤ê¸°
# for userId in user_list:
#     user_df = pd.DataFrame()
#     for graphName in graph_list:
#             invest_url = f"http://43.203.175.69:8002/api/invest/{graphName}/week?userId={userId}"
#             headers = {"Content-Type": "application/json"}

#             response = requests.get(invest_url, headers=headers)

#             if response.status_code == 200:
#                 # print(response.json())  # ì‘ë‹µ ë°ì´í„°
#                 data = response.json()  # JSON -> Python ê°ì²´ (list of dict)
#                 df = pd.DataFrame(data)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ DataFrameìœ¼ë¡œ ë³€í™˜
#                 print(df.head())  # í™•ì¸ìš©
#                 if df.empty:
#                     print(f"ğŸ“­ [EMPTY] userId: {userId}, graph: {graphName}")
#                     continue  # ì•„ë¬´ê²ƒë„ ì•ˆí•˜ê³  ë‹¤ìŒìœ¼ë¡œ

#                 print(f"âœ… [RECEIVED] userId: {userId}, graph: {graphName}, rows: {len(df)}")

#                 # ë³‘í•© ì²˜ë¦¬
#                 if user_df.empty:
#                     user_df = df
#                 else:
#                     user_df = pd.merge(user_df, df, on=["userId", "startedAt"], how="outer")

#             else:
#                 print(f"âŒ [ERROR] userId: {userId}, graph: {graphName}")
#                 print("ìƒíƒœ ì½”ë“œ:", response.status_code)

#                 # ì‘ë‹µì´ JSON í˜•ì‹ì´ë©´ íŒŒì‹±
#                 try:
#                     error_data = response.json()
#                     print("ğŸ” ì—ëŸ¬ ì‘ë‹µ(JSON):", error_data)
#                 except ValueError:
#                     # JSON í˜•ì‹ì´ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
#                     print("ğŸ” ì—ëŸ¬ ì‘ë‹µ(text):", response.text)
    
#     if not user_df.empty:
#         invest_merged_df = pd.concat([invest_merged_df, user_df], ignore_index=True)

# invest_merged_df = pd.read_csv("data/íŠ¹ì •ì‚¬ëŒ_invest_api_ë¶ˆëŸ¬ì™€ì„œ_ë³‘í•©.csv")

# # ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ
# invest_merged_df.to_csv("data/invest_merged_df.csv", index=False)
invest_merged_df = pd.read_csv("data/invest_merged_df.csv")
formatted_df = format_natural_language_summary(invest_merged_df)

# # ğŸ“ ì €ì¥
# formatted_df.to_csv("data/natural_format_data.csv", index=False)

# format_df = pd.read_csv("data/natural_format_data.csv")

documents = [
    Document(
        page_content=row["formatText"],
        metadata={"userId": row["userId"]}
    )
    for _, row in formatted_df.iterrows()
]

# Document(
#     page_content=f"""
#     ì‚¬ìš©ì ID: {row['userId']}
#     ê²Œì„ ì‹œì‘ ì‹œê°: {row['startedAt']}
#     ìš”ì•½ ì„¤ëª…: {row['formatText']}
#     """.strip(),
#     metadata={"userId": row["userId"]}
# )

embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# ë²¡í„° ì°¨ì› ì•Œì•„ë‚´ê¸°
embedding_dim = len(embedding.embed_query("ì„ë² ë”© í…ŒìŠ¤íŠ¸"))

# client.delete_collection(collection_name="my_user_summaries")

# ì¡°ê±´ë¶€ ìƒì„± (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
if not client.collection_exists("my_user_summaries"):
    client.create_collection(
        collection_name="my_user_summaries",
        vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)
    )

# ë¬¸ì„œ ìˆœíšŒí•˜ë©° ì—…ì„œíŠ¸
for doc in documents:
    user_id = doc.metadata["userId"]
    vector = embedding.embed_query(doc.page_content)
    
    # Qdrantì— ì—…ì„œíŠ¸
    client.upsert(
        collection_name="my_user_summaries",
        points=[
            PointStruct(
                id=user_id,  # ë¬¸ìì—´ UUIDë¥¼ ê·¸ëŒ€ë¡œ IDë¡œ ì‚¬ìš©
                vector=vector,
                payload={**doc.metadata, "text": doc.page_content}
            )
        ]
    )

# Qdrant ë²¡í„° DB ìƒì„± ë° ì €ì¥
qdrant = QdrantVectorStore(
    client=client,
    embedding=embedding,
    collection_name="my_user_summaries"
)

# qdrant.add_documents(documents)

# ë‚´ ì•„ì´ì˜ userId
target_user_id = "237aac1b-4d6f-4ca9-9e4f-30719ea5967d"

# Qdrantì—ì„œ í•´ë‹¹ userIdë¡œ point ì¡°íšŒ
# user_data = find_child_info(target_user_id, client)
# response = client.retrieve(
#     collection_name="my_user_summaries",
#     ids=[target_user_id] # ID ê¸°ë°˜ ì¡°íšŒ
# )

invest_df = invest_merged_df[invest_merged_df["userId"] == target_user_id]
print(invest_df)

template = """
ë„ˆëŠ” ì•„ì´ë“¤ì˜ ê²Œì„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ í–‰ë™ì„ íŒŒì•…í•˜ê³ , ê° ì˜ì—­ë³„ë¡œ í”¼ë“œë°±ê³¼ ê°€ì´ë“œë¥¼ ì œì‹œí•˜ëŠ” AI í•™ìŠµ ë¶„ì„ê°€ì•¼.

ë„ˆëŠ” í•­ìƒ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ í™œë™ ì˜ì—­ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´:
- íˆ¬ì(invest)
- ìƒì (shop)
- í€˜ìŠ¤íŠ¸(quest)

ê° ì˜ì—­ë³„ë¡œ ì œê³µëœ ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” **ìš”ì•½ í…ìŠ¤íŠ¸**ë¥¼ ê°ê° ë§Œë“¤ì–´ì¤˜:
1. í™œë™ëŸ‰ ë˜ëŠ” ë¹ˆë„
2. í–‰ë™ì˜ íŠ¹ì„± (ì˜ˆ: ê³ ìœ„í—˜ ì„ í˜¸, ìì£¼ ì†Œë¹„ ë“±)
3. ì „ì²´ í‰ê· ê³¼ì˜ ë¹„êµ
4. ê°œì„ í•  ì 
5. ì¹­ì°¬í•  ì 

ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë“  ì˜ì—­ì„ ì¢…í•©í•œ ë¶„ì„ ìš”ì•½ê³¼ ì•„ì´ì—ê²Œ ì í•©í•œ í•™ìŠµ/íˆ¬ì ìŠµê´€ ê°€ì´ë“œë¥¼ ìƒì„±í•´ì¤˜.

ğŸ”¹ ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒì˜ JSON êµ¬ì¡°ë¡œ í•´ì¤˜:
```json
{{
  "userId": "{user_id}",
  "invest": "íˆ¬ì ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "shop": "ìƒì  ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "quest": "í€˜ìŠ¤íŠ¸ ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
  "all": "ì „ì²´ í–‰ë™ ê²½í–¥ ë° í•™ìŠµ ê°€ì´ë“œ ìš”ì•½"
}}

[íˆ¬ì í™œë™ ë°ì´í„°]
{invest_data}

[ìƒì  í™œë™ ë°ì´í„°]
{shop_data}

[í€˜ìŠ¤íŠ¸ í™œë™ ë°ì´í„°]
{quest_data}
"""

prompt = PromptTemplate.from_template(template)

# ëª¨ë¸ ì •ì˜
llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    streaming=True,
    temperature=0.8,
    callbacks=[StreamingStdOutCallbackHandler()]
)

chain = LLMChain(prompt=prompt, llm=llm)

response = chain.run({
    "user_id": target_user_id,
    "invest_data": invest_df,
    "shop_data": shop_df,
    "quest_data": quest_df
})

print(response)