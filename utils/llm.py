from langchain_community.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
import os
import re
import json

def get_llm_chain():
  template = """
  ë„ˆëŠ” ì•„ì´ë“¤ì˜ ê²Œì„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ í–‰ë™ì„ íŒŒì•…í•˜ê³ , ê° ì˜ì—­ë³„ë¡œ **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë¹„êµë¥¼ í†µí•´ í”¼ë“œë°±ê³¼ ê°€ì´ë“œë¥¼ ì œì‹œí•˜ëŠ” AI í•™ìŠµ ë¶„ì„ê°€**ì•¼.

  ë¶„ì„ ëŒ€ìƒì€ í•­ìƒ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ í™œë™ ì˜ì—­ì´ì•¼:
  - íˆ¬ì(invest)
  - ìƒì (shop)
  - í€˜ìŠ¤íŠ¸(quest)

  ê° ì˜ì—­ì—ëŠ” ìˆ«ì ê¸°ë°˜ ì§€í‘œë“¤ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ, **ìˆ˜ì¹˜ì— ê¸°ë°˜í•˜ì—¬ ê°ê´€ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…**í•´ì¤˜. ì˜ˆë¥¼ ë“¤ì–´ 'í™œë™ëŸ‰ì´ 2.0ì´ê³  ì „ì²´ í‰ê· ì€ 1.5'ë¼ë©´, 'ì „ì²´ë³´ë‹¤ ì•½ 33% ë†’ì€ í™œë™ëŸ‰'ì´ë¼ê³  ì„œìˆ í•˜ëŠ” ì‹ì´ì•¼.

  ê° ì˜ì—­ë§ˆë‹¤ ì•„ë˜ í•­ëª©ì„ ëª¨ë‘ í¬í•¨í•œ **ìš”ì•½ í…ìŠ¤íŠ¸**ë¥¼ ìƒì„±í•´:
  1. í™œë™ëŸ‰ ë˜ëŠ” ë¹ˆë„ ìˆ˜ì¹˜ì™€ ì˜ë¯¸
  2. í–‰ë™ì˜ íŠ¹ì„± (ì˜ˆ: ê³ ìœ„í—˜ ì„ í˜¸, ì†Œë¹„ íŒ¨í„´, ì„±ê³µë¥  ë“±)
  3. ì „ì²´ í‰ê· ê³¼ ë¹„êµí•œ ìˆ˜ì¹˜ ë° í•´ì„
  4. ê°œì„ í•  ì  (ì™œ ê°œì„ ì´ í•„ìš”í•œì§€ë„ ê°„ë‹¨íˆ ì„¤ëª…)
  5. ì¹­ì°¬í•  ì  (ìˆ˜ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì¹­ì°¬)

  ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë“  ì˜ì—­ì„ ì¢…í•©í•´ì„œ:
  - ì•„ì´ì˜ **ì „ë°˜ì ì¸ í–‰ë™ ê²½í–¥**ì„ ì •ë¦¬í•˜ê³ ,
  - **í•™ìŠµ ë° ì¬ë¬´ ìŠµê´€ì— ëŒ€í•œ ì¢…í•© ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•´ì¤˜.

  ğŸ”¹ ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒì˜ JSON êµ¬ì¡°ë¡œ í•´ì¤˜. ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´.
  ```json
  {{
    "userId": "{user_id}",
    "invest": "íˆ¬ì ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
    "shop": "ìƒì  ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
    "quest": "í€˜ìŠ¤íŠ¸ ì˜ì—­ ë¶„ì„ ê²°ê³¼ ìš”ì•½",
    "all": "ì „ì²´ í–‰ë™ ê²½í–¥ ë° í•™ìŠµ ê°€ì´ë“œ ìš”ì•½"
  }}

  [íˆ¬ì í™œë™ ë°ì´í„°]
  {invest_data}

  [ìƒì  í™œë™ ë°ì´í„°]
  {shop_data}

  [í€˜ìŠ¤íŠ¸ í™œë™ ë°ì´í„°]
  {quest_data}
  """

  prompt = PromptTemplate.from_template(template)

  openai_key = os.getenv("OPENAI_KEY")

  # ëª¨ë¸ ì •ì˜
  llm = ChatOpenAI(
      model_name="gpt-4o-mini",
      streaming=True,
      temperature=0.8,
      openai_api_key=openai_key,
      callbacks=[StreamingStdOutCallbackHandler()]
  )

  return LLMChain(prompt=prompt, llm=llm)

def get_response(user_id, chain, invest_df, quest_df, shop_df):
  if invest_df.empty and quest_df.empty and shop_df.empty:
    print(f"ğŸ“­ ëª¨ë“  í™œë™ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (userId: {user_id})")
    return {
        "userId": user_id,
        "invest": "ë¶„ì„í•  íˆ¬ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "shop": "ë¶„ì„í•  ìƒì  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "quest": "ë¶„ì„í•  í€˜ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "all": "ëª¨ë“  ì˜ì—­ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™œë™ ë°ì´í„°ê°€ ìŒ“ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    }
  try:
    # ì²´ì¸ ì‹¤í–‰
    response = chain.run({
        "user_id": user_id,
        "invest_data": invest_df,
        "shop_data": shop_df,
        "quest_data": quest_df
    })

    # ì¶œë ¥ í´ë¦°ì—… ë° JSON íŒŒì‹±
    cleaned = re.sub(r"^```(?:json)?\n|\n```$", "", response.strip())
    response_user_json = json.loads(cleaned)

    return response_user_json
  
  except Exception as e:
    print(f"[ERROR] LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (userId={user_id}): {e}")
    return {
        "userId": user_id,
        "invest": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "shop": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "quest": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "all": "ë¶„ì„ ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    }